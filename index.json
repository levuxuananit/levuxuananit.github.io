[
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.1/",
	"title": "Generative AI",
	"tags": [],
	"description": "",
	"content": "Concept Generative AI, also known as Generative AI, is a field in artificial intelligence that focuses on creating new, complex, highly realistic content, including conversations, images, text, videos, etc. from existing data samples.\nYou can train Generative AI to learn many different fields such as human language, programming language, art, chemistry, biology, or any other complex field. This technology can learn from existing data and use that knowledge to create innovative products or solve specialized problems.\nGenerative AI began to develop strongly since 2010 when this technology helped improve language translation capabilities. In particular, by 2022, the American company OpenAI launched ChatGPT - a typical example of Generative AI and is considered a breakthrough in the field of artificial intelligence.\nA Deloitte survey of businesses globally found that more than 94% of executives believe that AI will significantly boost their business in the next five years. According to another study by Statista, the global Generative AI market is currently worth $44.89 billion and is expected to exceed $66 billion by the end of 2024.\nIt can be said that in the digital age Generative AI is an inevitable technology trend that promises to shape the future of businesses.\nBenefits Here are some of the key benefits of Generative AI:\nAccelerate the research process\nEnhance customer experience\nOptimize business processes\nIncrease employee productivity\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.1/",
	"title": "Initialize a Project",
	"tags": [],
	"description": "",
	"content": "Initialize a Project Download the CloudFormation Template: ws-startup-stack\nSave the YAML template file to a folder on your local machine.\nNavigate to the CloudFormation page: CloudFormation.\nOn the CloudFormation console, select Upload Template File.\nSelect the template you just downloaded and select Next.\nName the stack bedrock-workshop-environment For the Stack Configuration option, leave the default values ​​and select Next.\nSelect Submit to Deploy the Template.\nThe template deployment takes 10–15 minutes to complete all AWS resource provisioning.\nCongratulations on your success in getting here! Welcome to the next part of the workshop. "
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/",
	"title": "Knowledge",
	"tags": [],
	"description": "",
	"content": "Technology This workshop uses the Retrieval-Augmented Generation (RAG) technique to build a generative AI-powered chatbot. The Large Language Model (LLM) in Amazon Bedrock is used to answer your chatbot\u0026rsquo;s questions through pre-indexed content.\nAmazon Bedrock is a service that makes LLMs (from Amazon and leading AI startups) available via API, so you can choose from a variety of LLMs to find the model that best fits your use case. To store (index) and retrieve relevant content, you use Amazon Kendra, a fully managed service that provides intelligent enterprise searches powered by machine learning.\nIn addition, this Workshop uses Serverless technology, AWS Lambda, as the serverless computing to run the application.\nArchitecture Process The user asks a question to the chatbot through the application hosted through AWS Amplify. The user can have a general chat or a RAG-based chat by asking a question. The question is sent to the AWS Lambda function (RAG or LLM) through Amazon API Gateway. For the general chat, it returns a response to a given question. For the RAG chat, to retrieve relevant context from the document source, the RAG function calls the Amazon Kendra API. The RAG function then sends the API response, along with the built-in prompt, to the large language model (LLM) in Amazon Bedrock. The LLM\u0026rsquo;s response is then sent back to the user. Users can also perform reminder engineering operations. "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/",
	"title": "Practice",
	"tags": [],
	"description": "",
	"content": "Practice Content Initializing the Project\nSetting Up the Environment\nDeploying an Amazon Bedrock Serverless Application\nUsing the Amazon Bedrock Console\nGeneral Chat with Multiple LLMs\nIndexing Source Data Using Amazon Kendra\nRAG Chat with Multiple LLMs\nRapid Engineeringv\n"
},
{
	"uri": "http://levuxuananit.github.io/",
	"title": "Serverless chatbot",
	"tags": [],
	"description": "",
	"content": "BUILD SERVERLESS CHATBOT WITH AMAZON BEDROCK, AMAZON KENDRA AND PERSONAL DATA General introduction In today\u0026rsquo;s digital age, chatbots are becoming an important and popular technology in many fields thanks to their ability to automatically interact with humans via text or voice. Chatbots not only help optimize business performance but also bring a smooth and convenient user experience. This workshop will help you create your own Serverless chatbot using AWS services.\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.5/2.5.1/",
	"title": "Update model logic code",
	"tags": [],
	"description": "",
	"content": "Update logic code Open the llmfunction.py file, copy the code below and update it. The code below contains the logic to support the Claud3 (Haiku, Sonnet, etc.), Mistral, and Llama models. import os import json import boto3 from langchain_community.retrievers import AmazonKendraRetriever from langchain_aws import ChatBedrock from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate from langchain.chains import RetrievalQA import traceback kendra = boto3.client(\u0026#39;kendra\u0026#39;) chain_type = \u0026#39;stuff\u0026#39; KENDRA_INDEX_ID = os.getenv(\u0026#39;KENDRA_INDEX_ID\u0026#39;) S3_BUCKET_NAME = os.environ[\u0026#34;S3_BUCKET_NAME\u0026#34;] refine_prompt_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;This is the original question: {question}\\n\u0026#34; \u0026#34;The existing answer: {existing_answer}\\n\u0026#34; \u0026#34;Now there are some additional texts, (if needed) you can use them to improve your existing answer.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;{context_str}\\n\u0026#34; \u0026#34;\\\\nn\u0026#34; \u0026#34;Please use the new passage to further improve your answer.\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) initial_qa_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;The following is background knowledge：\\n\u0026#34; \u0026#34;{context_str}\u0026#34; \u0026#34;\\n\u0026#34; \u0026#34;Please answer this question based on the background knowledge provided above：{question}。\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) def lambda_handler(event, context): print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) question = event_body[\u0026#34;query\u0026#34;] print(f\u0026#34;Query is: {question}\u0026#34;) model_id = event_body[\u0026#34;model_id\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: llm = get_mistral_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/mistral-prompt-template.txt\u0026#39; elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: llm = get_llama_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/llama-prompt-template.txt\u0026#39; else: llm = get_claude_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; # Read the prompt template from S3 bucket s3 = boto3.resource(\u0026#39;s3\u0026#39;) obj = s3.Object(S3_BUCKET_NAME, PROMPT_TEMPLATE) prompt_template = obj.get()[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(f\u0026#34;prompt template: {prompt_template}\u0026#34;) retriever = AmazonKendraRetriever(kendra_client=kendra,index_id=KENDRA_INDEX_ID) if chain_type == \u0026#34;stuff\u0026#34;: PROMPT = PromptTemplate( template=prompt_template, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) chain_type_kwargs = {\u0026#34;prompt\u0026#34;: PROMPT} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) elif chain_type == \u0026#34;refine\u0026#34;: refine_prompt = PromptTemplate( input_variables=[\u0026#34;question\u0026#34;, \u0026#34;existing_answer\u0026#34;, \u0026#34;context_str\u0026#34;], template=refine_prompt_template, ) initial_qa_prompt = PromptTemplate( input_variables=[\u0026#34;context_str\u0026#34;, \u0026#34;question\u0026#34;], template=prompt_template, ) chain_type_kwargs = {\u0026#34;question_prompt\u0026#34;: initial_qa_prompt, \u0026#34;refine_prompt\u0026#34;: refine_prompt} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;refine\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) print(\u0026#39;Response\u0026#39;, response) source_documents = response.get(\u0026#39;source_documents\u0026#39;) source_docs = [] previous_source = None previous_score = None response_data = [] #if chain_type == \u0026#34;stuff\u0026#34;: for source_doc in source_documents: source = source_doc.metadata[\u0026#39;source\u0026#39;] score = source_doc.metadata[\u0026#34;score\u0026#34;] if source != previous_source or score != previous_score: source_data = { \u0026#34;source\u0026#34;: source, \u0026#34;score\u0026#34;: score } response_data.append(source_data) previous_source = source previous_score = score response_with_metadata = { \u0026#34;answer\u0026#34;: response.get(\u0026#39;result\u0026#39;), \u0026#34;source_documents\u0026#34;: response_data } return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response_with_metadata) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(f\u0026#34;stack trace: {stack_trace}\u0026#34;) print(f\u0026#34;error: {str(e)}\u0026#34;) response = str(e) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: response}) } def get_claude_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.95 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_llama_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_mistral_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_memory(): memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;answer\u0026#34;, return_messages=True ) return memory "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.7/2.7.1/",
	"title": "Updating the RAG Lambda Function",
	"tags": [],
	"description": "",
	"content": "Updating In this section, you will update and deploy the RAG Lambda function, which enables contextual conversation with many major language models.\nOpen the AWS Cloud9 environment and click Open link.\nOpen the ragfunction.py function, copy the code below, and update the function code. This function contains the logic to support the Claud3 (Haiku, Sonnet, etc.), Mistral, and Llama models.\nimport os import json import boto3 from langchain_community.retrievers import AmazonKendraRetriever from langchain_aws import ChatBedrock from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate from langchain.chains import RetrievalQA import traceback kendra = boto3.client(\u0026#39;kendra\u0026#39;) chain_type = \u0026#39;stuff\u0026#39; KENDRA_INDEX_ID = os.getenv(\u0026#39;KENDRA_INDEX_ID\u0026#39;) S3_BUCKET_NAME = os.environ[\u0026#34;S3_BUCKET_NAME\u0026#34;] refine_prompt_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;This is the original question: {question}\\n\u0026#34; \u0026#34;The existing answer: {existing_answer}\\n\u0026#34; \u0026#34;Now there are some additional texts, (if needed) you can use them to improve your existing answer.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;{context_str}\\n\u0026#34; \u0026#34;\\\\nn\u0026#34; \u0026#34;Please use the new passage to further improve your answer.\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) initial_qa_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;The following is background knowledge：\\n\u0026#34; \u0026#34;{context_str}\u0026#34; \u0026#34;\\n\u0026#34; \u0026#34;Please answer this question based on the background knowledge provided above：{question}。\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) def lambda_handler(event, context): print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) question = event_body[\u0026#34;query\u0026#34;] print(f\u0026#34;Query is: {question}\u0026#34;) model_id = event_body[\u0026#34;model_id\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: llm = get_mistral_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/mistral-prompt-template.txt\u0026#39; elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: llm = get_llama_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/llama-prompt-template.txt\u0026#39; else: llm = get_claude_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; # Read the prompt template from S3 bucket s3 = boto3.resource(\u0026#39;s3\u0026#39;) obj = s3.Object(S3_BUCKET_NAME, PROMPT_TEMPLATE) prompt_template = obj.get()[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(f\u0026#34;prompt template: {prompt_template}\u0026#34;) retriever = AmazonKendraRetriever(kendra_client=kendra,index_id=KENDRA_INDEX_ID) if chain_type == \u0026#34;stuff\u0026#34;: PROMPT = PromptTemplate( template=prompt_template, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) chain_type_kwargs = {\u0026#34;prompt\u0026#34;: PROMPT} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) elif chain_type == \u0026#34;refine\u0026#34;: refine_prompt = PromptTemplate( input_variables=[\u0026#34;question\u0026#34;, \u0026#34;existing_answer\u0026#34;, \u0026#34;context_str\u0026#34;], template=refine_prompt_template, ) initial_qa_prompt = PromptTemplate( input_variables=[\u0026#34;context_str\u0026#34;, \u0026#34;question\u0026#34;], template=prompt_template, ) chain_type_kwargs = {\u0026#34;question_prompt\u0026#34;: initial_qa_prompt, \u0026#34;refine_prompt\u0026#34;: refine_prompt} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;refine\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) print(\u0026#39;Response\u0026#39;, response) source_documents = response.get(\u0026#39;source_documents\u0026#39;) source_docs = [] previous_source = None previous_score = None response_data = [] #if chain_type == \u0026#34;stuff\u0026#34;: for source_doc in source_documents: source = source_doc.metadata[\u0026#39;source\u0026#39;] score = source_doc.metadata[\u0026#34;score\u0026#34;] if source != previous_source or score != previous_score: source_data = { \u0026#34;source\u0026#34;: source, \u0026#34;score\u0026#34;: score } response_data.append(source_data) previous_source = source previous_score = score response_with_metadata = { \u0026#34;answer\u0026#34;: response.get(\u0026#39;result\u0026#39;), \u0026#34;source_documents\u0026#34;: response_data } return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response_with_metadata) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(f\u0026#34;stack trace: {stack_trace}\u0026#34;) print(f\u0026#34;error: {str(e)}\u0026#34;) response = str(e) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: response}) } def get_claude_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.95 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_llama_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_mistral_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_memory(): memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;answer\u0026#34;, return_messages=True ) return memory "
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.3/1.3.1/",
	"title": "Variational self-encoding model",
	"tags": [],
	"description": "",
	"content": "Variational Autoencoders (VAEs) Concept: VAEs – Variational Autoencoders are a generative AI model that compresses and decodes data to create new versions. This model consists of two main parts:\nEncoder: The encoder takes input data and compresses it into a more compact representation, called the latent space. The latent space is a mathematical representation used to encode the key features of the data. For example, in facial research, it will contain numeric values ​​representing the shape of the eyes, nose, cheekbones, etc.\nDecoder: The decoder will reconstruct the original data or create new data from the latent space. This process allows the creation of new versions of the data while maintaining the key features of the original data.\nAdvantages: Generates data faster, useful for many types of applications\nDisadvantages: Generated data is often less detailed than diffusion models\n"
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.3/1.3.2/",
	"title": "Generative Adversarial Networks",
	"tags": [],
	"description": "",
	"content": "Generative Adversarial Networks (GANs) Concept: GANs – Generative Adversarial Networks were developed in 2014, and are another generative AI model built on the diffusion model. This model consists of two neural networks: generator and discriminator. The generative network generates fake data samples, while the discriminator tries to distinguish between real data (from the original data) and fake data (generated by the generative network).\nDuring training, the generative network continuously improves its ability to generate more real-like data, while the discriminator network gets better at distinguishing between real and fake. This adversarial process continues until the generative network can generate data that the discriminator cannot distinguish from real data.\nAdvantages: Can generate high quality models\nDisadvantages: The variety of models generated can be limited, more suitable for specific data\n"
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.2/",
	"title": "Generative AI &amp; Traditional AI",
	"tags": [],
	"description": "",
	"content": "Differentiation Traditionally, AI models perform natural language tasks (such as text generation, sentiment detection, question answering, and text classification) and image-based tasks (such as object detection and semantic segmentation).\nThese traditional models need to be trained with labeled datasets for a specific task each time. Generative AI models, on the other hand, are trained with large amounts of unlabeled data once and can be reused for different tasks. Therefore, generative models have more linguistic knowledge and demonstrate a low-pass learning capability that is unprecedented in traditional models. With just a few demonstrations, these models can generate coherent text in a new style or on a new topic.\nHow it works For Generative AI to interact with FMs for generative AI is through prompts. Prompts are primarily used to communicate with text-to-text FMs. In prompts, we provide instructions to FMs to generate new content. Writing good prompts helps get the best response from FMs. Examples of prompts include questions, instructions, or detailed descriptions. Prompting techniques can include phrasing queries, specifying a certain output type, providing relevant context, and assigning roles (such as “acting like a human assistant”) to the AI. You will learn more about prompting techniques in this workshop.\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.5/2.5.2/",
	"title": "Prerequisites and Start",
	"tags": [],
	"description": "",
	"content": "Prerequisites To run this lab, you\u0026rsquo;ll need an AWS account, and a user identity with access to the following services:\nAmazon DynamoDB AWS Cloud9 Environment You can use your own account, or an account provided through Workshop Studio Event Delivery as part of an AWS organized workshop. Using an account provided by Workshop Studio is the easier path, as you will have full access to all AWS services, and the account will terminate automatically when the event is over.\nAccount setup Using an account provided to you by your lab instructor If you are running this workshop using a link provided to you by your AWS instructor, please use that link and enter the access-code provided to you as part of the workshop. In the lab AWS account, the Cloud9 instance should already be provisioned. Please open the \u0026ldquo;AWS Cloud9\u0026rdquo; section of the AWS Management Console in the correct region and look for a lab instance called DynamoDBC9.\nUsing your own AWS account If you are using your own AWS account, be sure you have access to create and manage resources in Amazon DynamoDB and AWS Cloud9 environment\nAfter completing the workshop, remember to complete the cleanup section to remove any unnecessary AWS resources.\nLaunch the CloudFormation stack During the course of the lab, you will make DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. Ensure you delete the DynamoDB tables using the DynamoDB console, and make sure you delete the CloudFormation stack as soon as the lab is complete\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next. Scroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Create stack. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue to the next step.\nAccess EC2 to search for the Instance that was created from the stack. Execute \u0026ldquo;Connect to instance.\u0026rdquo; Run the command aws sts get-caller-identity just to verify that your AWS credentials have been configured correctly. "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.7/2.7.2/",
	"title": "Prerequisites and Start",
	"tags": [],
	"description": "",
	"content": "Prerequisites To run this lab, you\u0026rsquo;ll need an AWS account, and a user identity with access to the following services:\nAmazon DynamoDB AWS Cloud9 Environment You can use your own account, or an account provided through Workshop Studio Event Delivery as part of an AWS organized workshop. Using an account provided by Workshop Studio is the easier path, as you will have full access to all AWS services, and the account will terminate automatically when the event is over.\nAccount setup Using an account provided to you by your lab instructor If you are running this workshop using a link provided to you by your AWS instructor, please use that link and enter the access-code provided to you as part of the workshop. In the lab AWS account, the Cloud9 instance should already be provisioned. Please open the \u0026ldquo;AWS Cloud9\u0026rdquo; section of the AWS Management Console in the correct region and look for a lab instance called DynamoDBC9.\nUsing your own AWS account If you are using your own AWS account, be sure you have access to create and manage resources in Amazon DynamoDB and AWS Cloud9 environment\nAfter completing the workshop, remember to complete the cleanup section to remove any unnecessary AWS resources.\nLaunch the CloudFormation stack During the course of the lab, you will make DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. Ensure you delete the DynamoDB tables using the DynamoDB console, and make sure you delete the CloudFormation stack as soon as the lab is complete\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next. Scroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Create stack. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue to the next step.\nAccess EC2 to search for the Instance that was created from the stack. Execute \u0026ldquo;Connect to instance.\u0026rdquo; Run the command aws sts get-caller-identity just to verify that your AWS credentials have been configured correctly. "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.2/",
	"title": "Setting up the environment",
	"tags": [],
	"description": "",
	"content": "Setting Up the Environment To set up the environment for the workshop, open the AWS Cloud9 environment, as detailed in the following instructions.\nNavigate to the page: AWS Cloud9.\nIn the Environments section, select bedrock-workshop-environment-c9, and under Cloud9 IDE, select Open.\nEstablish the Environment You will need the Stack name to perform the following steps. You can do the following:\nNavigate to AWS CloudFormation\nRetrieve the Stack name as shown.\nOr you can copy the name of Stack name here: bedrock-workshop-environment\nOnce you get the Stack name, replace with the copied name and run the following command. Make sure to configure the environment variable S3BucketName, this bucket is used to store workshop samples and tests. export CFNStackName = \u0026lt;your-startup-stack-name\u0026gt; export S3BucketName=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;S3BucketName\u0026#39;].OutputValue\u0026#34; --output text) echo \u0026#34;S3 bucket name: $S3BucketName\u0026#34; Run the above commands in the Cloud9 IDE environment Congratulations on your success in getting here! Welcome to the next part of the workshop. "
},
{
	"uri": "http://levuxuananit.github.io/3-clean-up/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Clean Up Resources After completing the workshop, to prevent incurring additional charges, you should delete the resources you created in your AWS account.\nNavigate to the AWS Cloud9 site, then run the following commands: cd ~/environment/bedrock-serverless-workshop sam delete To delete the SAM stack, run the following command. aws cloudformation delete-stack --stack-name $CFNStackName "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.3/",
	"title": "Deploy an Amazon Bedrock Serverless Application",
	"tags": [],
	"description": "",
	"content": "In this task, you go through the steps to build and deploy the backend services. The deployment creates a serverless application, where the UI makes REST-based API calls to the Amazon Bedrock API. Once the code is deployed, it launches the Amazon API Gateway and an AWS Lambda function. To set up your environment.\nDeploy the application Open the AWS Cloud9 environment.\nClone the source code, and run the following command.\ncd ~/environment git clone https://github.com/aws-samples/bedrock-serverless-workshop.git Upgrade and Build the App To upgrade and build the backend and frontend of the serverless app, run the following command. cd ~/environment/bedrock-serverless-workshop chmod +x startup.sh ./startup.sh This script helps perform the following tasks: Upgrade the operating system and install the iq software.\nBuild the backend using sam build.\nDeploy the backend using sam deploy.\nInstall Amplify and build the frontend.\nPublish the frontend using Amplify.\nThe script will take between 2 and 5 minutes to complete. If at any point a git warning popup appears, select OK.\nDuring the amplify add host process, you will be prompted to select twice, keep the default selections and press enter. The default selections are: Hosting with Amplify Console (Managed hosting with custom domains, Continuous deployment)\nManual deployment\nOnce complete, you will see the following image. Copy and save the values ​​for the amplifyapp URL, user_id and password in a text editor. You use these credentials to log in to the system interface. amplifyapp: The URL path that helps you launch the system interface.\nuser_id and password: Help you log in to the system.\nThis interface is not yet ready with source documents and the chat feature is not yet working. In the next task, you will be guided through the process of indexing the source documents and testing them with sample questions\nBefore moving on to the next task, run the commands below, which are the environment variables required for the SAM and Amplify build commands for the rest of the lab. export SAMStackName=\u0026#34;sam-$CFNStackName\u0026#34; export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) export KendraIndexID=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;KendraIndexID\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CognitoUserPool\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolClientId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CongnitoUserPoolClientID\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) Congratulations on your success in getting here! Welcome to the next part of the workshop. "
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.3/1.3.3/",
	"title": "Diffusion Model",
	"tags": [],
	"description": "",
	"content": "Diffusion Models Concept: Diffusion models, also known as diffusion probabilistic models (DDPMs), are a type of generative model. They work in two steps: forward diffusion and reverse denoising.\nAdvantages: Diffusion models can generate very complex and high-quality classes\nDisadvantages: Training and generating new data can take a long time\nDuring diffusion, tiny random changes (noise) are added to the data for gradual training. Denoising removes this noise to create a new data sample that is similar to the original. The model starts with completely random noise and gradually removes it to create new high-quality data.\n"
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.3/",
	"title": "Typical models",
	"tags": [],
	"description": "",
	"content": "This workshop will introduce you to 4 typical models of generative AI including:\nVariational Autoencoders (VAEs)\nGenerative Adversarial Networks (GANs)\nDiffusion Models\nRetrieval Augmentation Models (RAGs)\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.4/",
	"title": "Amazon Bedrock Console User Guide",
	"tags": [],
	"description": "",
	"content": "Open Amazon Bedrock Console Log in to AWS Management Console, and then type Bedrock in the console search box. You are directed to a page that looks like this.\nEnable Model Access By default, your AWS account does not have model access, and you need access to the Claude3, Mistral, and Llama models to complete this workshop.\nIn the upper-left corner of the Amazon Bedrock console, select the menu icon (hamburger). In the navigation pane, select Model Access, and then select Enable Specific Models.\nSelect the Claude 3 Sonnet, Claude 3 Haiku, Llama 3 Chat 13B, and Mistral 7B Instruct models.\nSelect Next.\nSelect Submit.\nSet up the model In the upper left corner of the Amazon Bedrock dashboard, select the menu icon. Under Playgrounds, select Chat.\nUnder Playgrounds, select Select model.\nUnder Category, select Anthropic and select Claude 3 Haiku.\nSelect Apply to apply the model.\nAsk a question The Chat playground\u0026quot; interface opens. In the prompt text box, enter the question, then select Run. For example: List the 10 most populous cities in the United States?\nThe model analyzes and returns the response as shown below.\nCongratulations on your success in getting here! Welcome to the next part of the workshop. "
},
{
	"uri": "http://levuxuananit.github.io/1-introduction/1.3/1.3.4.1/",
	"title": "Enhanced Access Model",
	"tags": [],
	"description": "",
	"content": "Retrieval Augmented Generation (RAG) Model Concept: RAG - Retrieval-Augmented Generation is a model architecture that combines two main components: retrieval and generation. It is designed to improve the output quality of a language model by retrieving relevant documents or data from a knowledge base before generating text. RAG is commonly used in question answering, knowledge retrieval, and other intelligent AI systems.\nRetrieval (Information retrieval): When receiving input (e.g., a question), the system searches for relevant documents or text passages from a database or information retrieval system.\nGeneration (Text generation): After retrieving documents, the system uses a language model (e.g., GPT, BERT) to generate output text based on the retrieved information.\nAdvantages: RAG combines data retrieval with document generation, allowing the model to synthesize information from external sources, improving accuracy and up-to-dateness. It helps reduce bias, avoids incorrect inferences, and does not require retraining when new data is available. This makes RAG effective in Q\u0026amp;A systems and big data processing.\nDisadvantages: RAG depends on the quality of the retrieved data, which can lead to incorrect answers if the data is inaccurate. It also requires high computational costs, increasing complexity and latency. Furthermore, if the database is not well managed, the system may have difficulty retrieving relevant information and maintaining consistency when documents are inconsistent.\nBy interacting with Generative AI FMs through prompts. Prompts are primarily used to communicate with text-to-text FMs. In prompts, we provide instructions to FMs to create new content.\nWriting good prompts helps get the best responses from FMs. Examples of prompts include questions, instructions, or detailed descriptions. Prompting techniques can include phrasing queries, specifying a certain output type, providing relevant context, and assigning roles (such as “acting like a human assistant”) to the AI. You will explore more about prompting techniques in this workshop.\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.5/",
	"title": "General Chat with Multiple LLMs",
	"tags": [],
	"description": "",
	"content": "General Chat with Multiple LLMs You have successfully deployed an Amazon Bedrock serverless chatbot application, using AWS SAM for the backend and AWS Amplify for the frontend. The frontend serves as a basic user interface to test the solution with various questions and prompts. In this section, you will update and deploy your LLM Lambda function, which enables general chat with multiple major language models.\nOpen your AWS Cloud9 environment and click Open link. "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.6/",
	"title": "Indexing Source Data Using Amazon Kendra",
	"tags": [],
	"description": "",
	"content": "In a RAG architecture, Amazon Kendra can be leveraged to index and search through a collection of sample documents stored in Amazon S3 and other sources. You can provide a query or ask a question, and Kendra will perform a similarity search across the entire indexed content to identify the most relevant information.\nKendra\u0026rsquo;s advanced natural language processing capabilities allow it to understand your intent and query, and then retrieve the most relevant content from indexed data sources. This allows you to quickly find the information you need without having to manually sift through large volumes of documents.\nBy integrating Kendra into the “Retrieve” phase of the RAG architecture, organizations can enhance their overall information search and discovery capabilities, ultimately supporting more efficient analysis and generating insights and responses. Kendra’s seamless integration with Amazon S3 simplifies the underlying content indexing and management process, making it a powerful tool within the RAG framework.\nInstalling sample documents Install some sample documents to test the solution. There are 03 sample documents: Document about the September 2023 Federal Open Market Committee (FOMC) meeting minutes.\nDocument about Amazon’s 2022 Sustainability Report.\nDocument about the 10K report of Henry Schein, a dental service provider.\nYou can download or use any of the documents to test this solution, or you can bring your own data to test.\nTo copy the reminder templates and sample documents you downloaded to your S3 bucket, run the following command. bash cd ~/environment/bedrock-serverless-workshop mkdir sample-documents curl https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20230920.pdf --output sample-documents/fomcminutes20230920.pdf curl https://sustainability.aboutamazon.com/2022-sustainability-report.pdf --output sample-documents/2022-sustainability- report-amazon.pdf curl https://investor.henryschein.com/static-files/fcf569ec-fdbb-4591-b73d-0d1d849efd78 --output sample-documents/2022-hs1-10k.pdf 3. The command will execute as shown below Installing the Templates and Reminders To double-check that there is no previous interruption in the process, run the following command (where is your Stack name) export CFNStackName = \u0026lt;your-startup-stack-name\u0026gt; export S3BucketName=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;S3BucketName\u0026#39;].OutputValue\u0026#34; --output text) echo \u0026#34;S3 bucket name: $S3BucketName\u0026#34; To copy the template and reminders you downloaded to your S3 bucket, run the following command. cd ~/environment/bedrock-serverless-workshop aws s3 cp sample-documents s3://$S3BucketName/sample-documents/ --recursive aws s3 cp prompt-engineering s3://$S3BucketName/prompt-engineering/ --recursive The above commands will execute as shown below After the upload is successful, review the Amazon S3 console and open the bucket. You should see the following content Indexing sample documents using Amazon Kendra The Amazon Kendra index and Amazon S3 data source were created during the initial provisioning for this workshop. In this task, you index all the documents in the S3 data source.\nSign in to the AWS Management Console, then type Kendra in the console search box. Check that your AWS account has access to Amazon Kendra, then select the AWS region where this workshop is taking place. Select Indexes\nIn the left pane, select Data sources\nTo start indexing all the documents from the sample-documents folder, select S3DocsDataSource, then select Sync now. Indexing may take a few minutes. Wait until it completes. To query the Amazon Kendra index with a few sample questions, in the left navigation pane, select Search indexed content, and then ask a question. Ask an example question, like: What is the federal funds rate as of April 2024 ? . And Amazon Kendra analyzes and returns the answer as shown below Congratulations on your success in getting here! Welcome to the next part of the workshop. "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.7/",
	"title": "RAG Chat with Multiple LLMs",
	"tags": [],
	"description": "",
	"content": "You have successfully deployed an Amazon Bedrock serverless chatbot application and enabled the model to access the required Large Language Models (LLMs) using the Amazon Bedrock console. Additionally, you have completed the Amazon Kendra setup.\nRAG-Based Solution with Amazon Bedrock and LangChain The solution in this workshop is built using the Retrieval-Augmented Generation (RAG) approach. RAG is a model architecture that integrates aspects of both retrieval and generation techniques to improve the quality and relevance of the generated text. When you enter a question in the question text box, the following steps are run to provide you with an answer derived from the document source:\nRetrieval: This process searches a large corpus of text data for relevant information or context. During this phase, Amazon Kendra takes the question from the request and searches for relevant answers and references.\nEnhanced: After retrieving relevant information, the model uses the retrieved context to augment the text generation. This means that the generated text is influenced by the retrieved information, ensuring that the generated content is contextual and informative.\nGenerated: Generation in RAG refers to the traditional generative aspect of the model, where the model generates new text based on the retrieved and augmented context. This generated text can be in the form of an answer, response, or explanation.\nLangChain: To orchestrate this stream, we use the LangChain agent in this workshop. LangChain\u0026rsquo;s flexible abstractions and comprehensive toolkit empower developers to tap into the power of platform models (FM).\nIn this task, you will implement a RAG (Retrieve, Analyze, Create) Lambda function to deliver a contextual chatbot experience with your datasets. The sample datasets are stored in Amazon S3 and indexed using Amazon Kendra. To orchestrate the stream between user queries, Kendra indexes, and LLMs, you will use LangChain as the orchestrator. The Lambda code provided uses the LangChain API to abstract away the complex logic required for this integration.\nBy leveraging the power of Amazon Bedrock, Amazon Kendra, and LangChain, you can create seamless, contextual chatbot experiences for your users, allowing them to interact with your data sets in a natural and efficient way.\nIn this section, you will update and deploy a RAG Lambda function that enables contextual conversation with multiple large language models.\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.8/",
	"title": "Quick Techniques",
	"tags": [],
	"description": "",
	"content": "Prompting Techniques Quick techniques are an important aspect of the Retrieval-Augmented Generation (RAG) architecture, as they play a critical role in each phase of the process. In the Retrieval phase, prompting techniques are used to generate efficient queries to retrieve the most relevant information from data sources, such as Amazon S3 documents indexed with Amazon Kendra, that accurately capture the user\u0026rsquo;s intent.\nIn the analysis phase, prompts guide the processing and extraction of insights from the retrieved data, specifying the type of analysis, level of detail, and desired output format. Finally, in the Generation phase, prompting is essential to leverage large language models like Claude 3 Sonnet to generate coherent, relevant, and task-specific responses that provide the necessary context, guidance, and direction. By mastering prompting, users can optimize the performance of the RAG architecture, ensuring that the retrieved information is relevant, the analysis is insightful, and the generated responses effectively address the user’s needs.\nIn this assignment, you will work to refine the prompt for the large language model Claude 3 Sonnet. You will experiment with how the response behavior changes based on the prompt you provide. Initially, you will ask a question with a loosely structured prompt, which can cause the LLM to stray from the context and produce unexpected results. As you refine the prompt, the response will become more consistent with the expected result and less likely to be an illusion. This part is still part of the RAG architecture and will use the same data source that you used in the previous task.\n"
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.8/2.8.1/",
	"title": "Experience",
	"tags": [],
	"description": "",
	"content": "Open the skill page quickly Go back to your browser and open the chatbot website. Click on the link Prompt Engineering. The website is displayed as follows Try a question below that is not related to the source documents you have indexed so far: Tell me a story about a fox and a tiger, the story must be for 5-year-olds and under 100 words.\nFor the prompt template, copy the following prompt and paste it into the Prompt Template text box. This is a loosely structured prompt without any specific instructions or guidance provided for the Claude 3 Sonnet model.\n{context} and {question} "
},
{
	"uri": "http://levuxuananit.github.io/2-hand-on/2.8/2.8.2/",
	"title": "Additional Prompt Structure",
	"tags": [],
	"description": "",
	"content": "Prompt Structure For the prompt template, copy the following prompt and paste it into the Prompt Template text box. This is a loosely structured prompt without any specific instructions or guidance provided for the Claude 3 Sonnet model. {context} and {question} Now, update the prompt with the following refined version. With this refined prompt, you can expect to get the desired output. Human: You are an intelligent AI advisor and provide answers to questions using fact-based information. Use the following information to provide a concise answer to the question included in the \u0026lt;question\u0026gt; tag. Look for contextual information included in the \u0026lt;context\u0026gt; tag. If you don\u0026#39;t know the answer, just say you don\u0026#39;t know, don\u0026#39;t try to make up an answer. \u0026lt;context\u0026gt;{context}\u0026lt;/context\u0026gt; \u0026lt;question\u0026gt;{question}\u0026lt;/question\u0026gt; Answers should be specific and use only factual information. "
},
{
	"uri": "http://levuxuananit.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://levuxuananit.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]