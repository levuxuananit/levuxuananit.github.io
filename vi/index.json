[
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.5/2.5.1/",
	"title": "Cập nhật code logic của mô hình",
	"tags": [],
	"description": "",
	"content": "Cập nhật code logic Mở file llmfunction.py, sao chép code bên dưới và cập nhật lại. Code bên dưới mang logic để hỗ trợ các mô hình Claud3 (Haiku, Sonnet, v.v.), Mistral, và Llama. import os import json import boto3 from langchain_community.retrievers import AmazonKendraRetriever from langchain_aws import ChatBedrock from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate from langchain.chains import RetrievalQA import traceback kendra = boto3.client(\u0026#39;kendra\u0026#39;) chain_type = \u0026#39;stuff\u0026#39; KENDRA_INDEX_ID = os.getenv(\u0026#39;KENDRA_INDEX_ID\u0026#39;) S3_BUCKET_NAME = os.environ[\u0026#34;S3_BUCKET_NAME\u0026#34;] refine_prompt_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;This is the original question: {question}\\n\u0026#34; \u0026#34;The existing answer: {existing_answer}\\n\u0026#34; \u0026#34;Now there are some additional texts, (if needed) you can use them to improve your existing answer.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;{context_str}\\n\u0026#34; \u0026#34;\\\\nn\u0026#34; \u0026#34;Please use the new passage to further improve your answer.\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) initial_qa_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;The following is background knowledge：\\n\u0026#34; \u0026#34;{context_str}\u0026#34; \u0026#34;\\n\u0026#34; \u0026#34;Please answer this question based on the background knowledge provided above：{question}。\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) def lambda_handler(event, context): print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) question = event_body[\u0026#34;query\u0026#34;] print(f\u0026#34;Query is: {question}\u0026#34;) model_id = event_body[\u0026#34;model_id\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: llm = get_mistral_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/mistral-prompt-template.txt\u0026#39; elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: llm = get_llama_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/llama-prompt-template.txt\u0026#39; else: llm = get_claude_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; # Read the prompt template from S3 bucket s3 = boto3.resource(\u0026#39;s3\u0026#39;) obj = s3.Object(S3_BUCKET_NAME, PROMPT_TEMPLATE) prompt_template = obj.get()[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(f\u0026#34;prompt template: {prompt_template}\u0026#34;) retriever = AmazonKendraRetriever(kendra_client=kendra,index_id=KENDRA_INDEX_ID) if chain_type == \u0026#34;stuff\u0026#34;: PROMPT = PromptTemplate( template=prompt_template, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) chain_type_kwargs = {\u0026#34;prompt\u0026#34;: PROMPT} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) elif chain_type == \u0026#34;refine\u0026#34;: refine_prompt = PromptTemplate( input_variables=[\u0026#34;question\u0026#34;, \u0026#34;existing_answer\u0026#34;, \u0026#34;context_str\u0026#34;], template=refine_prompt_template, ) initial_qa_prompt = PromptTemplate( input_variables=[\u0026#34;context_str\u0026#34;, \u0026#34;question\u0026#34;], template=prompt_template, ) chain_type_kwargs = {\u0026#34;question_prompt\u0026#34;: initial_qa_prompt, \u0026#34;refine_prompt\u0026#34;: refine_prompt} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;refine\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) print(\u0026#39;Response\u0026#39;, response) source_documents = response.get(\u0026#39;source_documents\u0026#39;) source_docs = [] previous_source = None previous_score = None response_data = [] #if chain_type == \u0026#34;stuff\u0026#34;: for source_doc in source_documents: source = source_doc.metadata[\u0026#39;source\u0026#39;] score = source_doc.metadata[\u0026#34;score\u0026#34;] if source != previous_source or score != previous_score: source_data = { \u0026#34;source\u0026#34;: source, \u0026#34;score\u0026#34;: score } response_data.append(source_data) previous_source = source previous_score = score response_with_metadata = { \u0026#34;answer\u0026#34;: response.get(\u0026#39;result\u0026#39;), \u0026#34;source_documents\u0026#34;: response_data } return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response_with_metadata) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(f\u0026#34;stack trace: {stack_trace}\u0026#34;) print(f\u0026#34;error: {str(e)}\u0026#34;) response = str(e) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: response}) } def get_claude_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.95 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_llama_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_mistral_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_memory(): memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;answer\u0026#34;, return_messages=True ) return memory "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.7/2.7.1/",
	"title": "Cập nhật hàm RAG Lambda",
	"tags": [],
	"description": "",
	"content": "Cập nhật Trong phần này, bạn sẽ cập nhật và triển khai hàm RAG Lambda, hàm này cho phép trò chuyện theo ngữ cảnh với nhiều mô hình ngôn ngữ lớn.\nhãy mở AWS Cloud9 environment và nhấp Open link.\nMở hàmragfunction.py, sao chép mã bên dưới và cập nhật mã hàm. Hàm này mang logic để hỗ trợ các mô hình Claud3 (Haiku, Sonnet, v.v.), Mistral và Llama.\nimport os import json import boto3 from langchain_community.retrievers import AmazonKendraRetriever from langchain_aws import ChatBedrock from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate from langchain.chains import RetrievalQA import traceback kendra = boto3.client(\u0026#39;kendra\u0026#39;) chain_type = \u0026#39;stuff\u0026#39; KENDRA_INDEX_ID = os.getenv(\u0026#39;KENDRA_INDEX_ID\u0026#39;) S3_BUCKET_NAME = os.environ[\u0026#34;S3_BUCKET_NAME\u0026#34;] refine_prompt_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;This is the original question: {question}\\n\u0026#34; \u0026#34;The existing answer: {existing_answer}\\n\u0026#34; \u0026#34;Now there are some additional texts, (if needed) you can use them to improve your existing answer.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;{context_str}\\n\u0026#34; \u0026#34;\\\\nn\u0026#34; \u0026#34;Please use the new passage to further improve your answer.\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) initial_qa_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;The following is background knowledge：\\n\u0026#34; \u0026#34;{context_str}\u0026#34; \u0026#34;\\n\u0026#34; \u0026#34;Please answer this question based on the background knowledge provided above：{question}。\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) def lambda_handler(event, context): print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) question = event_body[\u0026#34;query\u0026#34;] print(f\u0026#34;Query is: {question}\u0026#34;) model_id = event_body[\u0026#34;model_id\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: llm = get_mistral_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/mistral-prompt-template.txt\u0026#39; elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: llm = get_llama_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/llama-prompt-template.txt\u0026#39; else: llm = get_claude_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; # Read the prompt template from S3 bucket s3 = boto3.resource(\u0026#39;s3\u0026#39;) obj = s3.Object(S3_BUCKET_NAME, PROMPT_TEMPLATE) prompt_template = obj.get()[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(f\u0026#34;prompt template: {prompt_template}\u0026#34;) retriever = AmazonKendraRetriever(kendra_client=kendra,index_id=KENDRA_INDEX_ID) if chain_type == \u0026#34;stuff\u0026#34;: PROMPT = PromptTemplate( template=prompt_template, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) chain_type_kwargs = {\u0026#34;prompt\u0026#34;: PROMPT} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) elif chain_type == \u0026#34;refine\u0026#34;: refine_prompt = PromptTemplate( input_variables=[\u0026#34;question\u0026#34;, \u0026#34;existing_answer\u0026#34;, \u0026#34;context_str\u0026#34;], template=refine_prompt_template, ) initial_qa_prompt = PromptTemplate( input_variables=[\u0026#34;context_str\u0026#34;, \u0026#34;question\u0026#34;], template=prompt_template, ) chain_type_kwargs = {\u0026#34;question_prompt\u0026#34;: initial_qa_prompt, \u0026#34;refine_prompt\u0026#34;: refine_prompt} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;refine\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) print(\u0026#39;Response\u0026#39;, response) source_documents = response.get(\u0026#39;source_documents\u0026#39;) source_docs = [] previous_source = None previous_score = None response_data = [] #if chain_type == \u0026#34;stuff\u0026#34;: for source_doc in source_documents: source = source_doc.metadata[\u0026#39;source\u0026#39;] score = source_doc.metadata[\u0026#34;score\u0026#34;] if source != previous_source or score != previous_score: source_data = { \u0026#34;source\u0026#34;: source, \u0026#34;score\u0026#34;: score } response_data.append(source_data) previous_source = source previous_score = score response_with_metadata = { \u0026#34;answer\u0026#34;: response.get(\u0026#39;result\u0026#39;), \u0026#34;source_documents\u0026#34;: response_data } return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response_with_metadata) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(f\u0026#34;stack trace: {stack_trace}\u0026#34;) print(f\u0026#34;error: {str(e)}\u0026#34;) response = str(e) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: response}) } def get_claude_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.95 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_llama_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_mistral_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_memory(): memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;answer\u0026#34;, return_messages=True ) return memory "
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.1/",
	"title": "Generative AI",
	"tags": [],
	"description": "",
	"content": "Khái niệm Generative AI, còn được gọi là Generative AI, là một lĩnh vực trong trí tuệ nhân tạo tập trung vào việc tạo ra nội dung mới, phức tạp, có tính thực tế cao, bao gồm các cuộc hội thoại, hình ảnh, văn bản, video, v.v. từ các mẫu dữ liệu hiện có.\nBạn có thể đào tạo Generative AI để học nhiều lĩnh vực khác nhau như ngôn ngữ con người, ngôn ngữ lập trình, nghệ thuật, hóa học, sinh học hoặc bất kỳ lĩnh vực phức tạp nào khác. Công nghệ này có thể học hỏi từ dữ liệu hiện có và sử dụng kiến ​​thức đó để tạo ra các sản phẩm sáng tạo hoặc giải quyết các vấn đề chuyên biệt.\nGenerative AI bắt đầu phát triển mạnh mẽ từ năm 2010 khi công nghệ này giúp cải thiện khả năng dịch ngôn ngữ. Đặc biệt, đến năm 2022, công ty OpenAI của Mỹ đã ra mắt ChatGPT - một ví dụ điển hình của Generative AI và được coi là bước đột phá trong lĩnh vực trí tuệ nhân tạo.\nMột cuộc khảo sát của Deloitte về các doanh nghiệp trên toàn cầu cho thấy hơn 94% giám đốc điều hành tin rằng AI sẽ thúc đẩy đáng kể hoạt động kinh doanh của họ trong năm năm tới. Theo một nghiên cứu khác của Statista, thị trường Generative AI toàn cầu hiện có giá trị 44,89 tỷ đô la và dự kiến ​​sẽ vượt quá 66 tỷ đô la vào cuối năm 2024. Có thể nói rằng trong thời đại kỹ thuật số, Generative AI là xu hướng công nghệ tất yếu hứa hẹn sẽ định hình tương lai của các doanh nghiệp.\nLợi ích Sau đây là một số lợi ích chính của Generative AI: - Đẩy nhanh quá trình nghiên cứu - Nâng cao trải nghiệm của khách hàng - Tối ưu hóa quy trình kinh doanh - Tăng năng suất của nhân viên\nĐẩy nhanh quá trình nghiên cứu\nNâng cao trải nghiệm của khách hàng\nTối ưu hóa quy trình kinh doanh\nTăng năng suất của nhân viên\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.1/",
	"title": "Khởi tạo dự án",
	"tags": [],
	"description": "",
	"content": "Khởi tạo dự án Tải xuống Mẫu CloudFormation: ws-startup-stack\nLưu tệp mẫu YAML vào một thư mục trên máy cục bộ của bạn.\nĐiều hướng đến trang CloudFormation: CloudFormation.\nTrên bảng điều khiển CloudFormation, chọn Tải tệp mẫu lên.\nChọn mẫu bạn vừa tải xuống và chọn Tiếp theo.\nĐặt tên Stack: bedrock-workshop-environment Đối với tùy chọn Cấu hình ngăn xếp, hãy để nguyên các giá trị mặc định và chọn Tiếp theo.\nChọn Gửi để triển khai Mẫu.\nViệc triển khai mẫu mất 10–15 phút để hoàn tất toàn bộ việc cung cấp tài nguyên AWS.\nXin chúc mừng bạn đã thành công khi đến đây! Chào mừng đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/",
	"title": "Kiến thức",
	"tags": [],
	"description": "",
	"content": "Công nghệ Hội thảo này sử dụng kỹ thuật Retrieval-Augmented Generation (RAG) để xây dựng một chatbot hỗ trợ AI tạo sinh. Mô hình ngôn ngữ lớn (LLM) trong Amazon Bedrock được sử dụng để trả lời các câu hỏi chatbot của bạn thông qua nội dung được lập chỉ mục trước.\nAmazon Bedrock là một dịch vụ giúp LLM (từ Amazon và các công ty khởi nghiệp AI hàng đầu) có sẵn thông qua API, do đó, bạn có thể chọn từ nhiều LLM khác nhau để tìm mô hình phù hợp nhất với trường hợp sử dụng của mình. Để lưu trữ (lập chỉ mục) và truy xuất nội dung có liên quan, bạn sử dụng Amazon Kendra, một dịch vụ được quản lý hoàn toàn cung cấp các tìm kiếm doanh nghiệp thông minh được hỗ trợ bởi máy học.\nNgoài ra, Hội thảo này sử dụng công nghệ Serverless là AWS Lambda làm máy tính không có máy chủ để chạy ứng dụng.\nKiến trúc Tiến trình Người dùng đặt câu hỏi cho chatbot thông qua ứng dụng được lưu trữ thông qua AWS Amplify. Người dùng có thể trò chuyện chung hoặc trò chuyện dựa trên RAG bằng cách đặt câu hỏi. Câu hỏi được gửi đến hàm AWS Lambda (RAG hoặc LLM) thông qua Amazon API Gateway. Đối với trò chuyện chung, nó trả về phản hồi cho một câu hỏi nhất định. Đối với trò chuyện RAG, để truy xuất ngữ cảnh có liên quan từ nguồn tài liệu, hàm RAG gọi API Amazon Kendra. Sau đó, hàm RAG gửi nội dung trả về của API, cùng với lời nhắc được dựng sẵn, đến mô hình ngôn ngữ lớn (LLM) trong Amazon Bedrock. Sau đó, phản hồi trả về của LLM được gửi lại cho người dùng. Người dùng cũng có thể thực hiện các hoạt động kỹ thuật nhắc nhở. "
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.3/1.3.1/",
	"title": "Mô hình tự mã hóa biến phân",
	"tags": [],
	"description": "",
	"content": "Mô hình tự mã hóa biến phân (VAEs) Khái niệm: VAEs – Variational Autoencoders là một mô hình AI tạo sinh giúp nén và giải mã dữ liệu để tạo ra các phiên bản mới. Mô hình này gồm hai phần chính:\nBộ mã hóa (Encoder): Bộ mã hóa nhận dữ liệu đầu vào và nén nó thành một dạng biểu diễn nhỏ gọn hơn, gọi là không gian ngầm. Không gian ngầm là biểu diễn toán học dùng để mã hóa các đặc điểm chính của dữ liệu. Ví dụ, trong nghiên cứu khuôn mặt, nó sẽ chứa các giá trị số đại diện cho hình dạng mắt, mũi, gò má…\nBộ giải mã (Decoder): Bộ giải mã sẽ tái tạo lại dữ liệu gốc hoặc tạo ra dữ liệu mới từ không gian ngầm. Quá trình này cho phép tạo ra các phiên bản mới của dữ liệu mà vẫn duy trì các đặc điểm chính của dữ liệu gốc.\nƯu điểm: Tạo ra dữ liệu nhanh hơn, hữu ích cho nhiều loại ứng dụng\nNhược điểm: Dữ liệu tạo ra thường không chi tiết bằng mô hình khuếch tán\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/",
	"title": "Serverless chatbot",
	"tags": [],
	"description": "",
	"content": "XÂY DỰNG SERVERLESS CHATBOT BẰNG AMAZON BEDROCK, AMAZON KENDRA VÀ DỮ LIỆU CÁ NHÂN Giới thiệu chung Trong kỷ nguyên số hiện nay, chatbot đang trở thành một công nghệ quan trọng và phổ biến trong nhiều lĩnh vực nhờ khả năng tương tác tự động với con người thông qua văn bản hoặc giọng nói. Chatbot không chỉ giúp tối ưu hóa hiệu quả hoạt động của doanh nghiệp mà còn mang lại trải nghiệm người dùng mượt mà và thuận tiện. Hội thảo này sẽ giúp bạn tự tạo ra Serverless chatbot của riêng mình bằng các dịch vụ của AWS.\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/",
	"title": "Thực hành",
	"tags": [],
	"description": "",
	"content": "Nội dung thực hành Khởi tạo dự án\nThiết lập môi trường\nTriển khai ứng dụng Amazon Bedrock Serverless\nHướng dẫn sử dụng Amazon Bedrock Console\nTrò chuyện chung với nhiều LLM\nLập chỉ mục dữ liệu nguồn bằng cách sử dụng Amazon Kendra\nTrò chuyện RAG với nhiều LLM\nKỹ thuật nhanh chóng\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.2/",
	"title": "AI tạo sinh &amp; AI truyền thống",
	"tags": [],
	"description": "",
	"content": "Phân biệt Theo truyền thống, các mô hình AI thực hiện các tác vụ ngôn ngữ tự nhiên (như tạo văn bản, phát hiện tình cảm, trả lời câu hỏi và phân loại văn bản) và các tác vụ dựa trên hình ảnh (như phát hiện đối tượng và phân đoạn ngữ nghĩa).\nCác mô hình truyền thống này cần được đào tạo bằng các tập dữ liệu được gắn nhãn cho một tác vụ cụ thể mỗi lần. Mặt khác, các mô hình AI tạo sinh được đào tạo bằng một lượng lớn dữ liệu không được gắn nhãn một lần và chúng có thể được sử dụng lại cho các tác vụ khác nhau. Do đó, các mô hình tạo sinh có nhiều kiến ​​thức ngôn ngữ hơn và thể hiện khả năng học ít lần chưa từng thấy ở các mô hình truyền thống. Chỉ với một vài lần trình diễn, các mô hình này có thể tạo ra văn bản mạch lạc theo một phong cách mới hoặc về một chủ đề mới.\nCách hoạt động Đối với Generative AI tương tác với các FM cho AI tạo sinh thông qua lời nhắc. Lời nhắc chủ yếu được sử dụng để giao tiếp với các FM chuyển văn bản thành văn bản. Trong lời nhắc, chúng tôi cung cấp hướng dẫn cho FM để tạo nội dung mới. Viết lời nhắc tốt giúp nhận được phản hồi tốt nhất từ ​​các FM. Ví dụ về lời nhắc bao gồm câu hỏi, hướng dẫn hoặc mô tả chi tiết. Kỹ thuật lời nhắc có thể bao gồm việc diễn đạt truy vấn, chỉ định một kiểu đầu ra nhất định, cung cấp bối cảnh có liên quan và chỉ định vai trò (chẳng hạn như \u0026ldquo;hành động như một trợ lý con người\u0026rdquo;) cho AI. Bạn sẽ khám phá thêm về kỹ thuật lời nhắc trong hội thảo này.\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.3/1.3.2/",
	"title": "Mạng đối nghịch tạo sinh",
	"tags": [],
	"description": "",
	"content": "Mạng đối nghịch tạo sinh (GANs) Khái niệm: GANs – Generative Adversarial Networks được phát triển vào năm 2014, là một mô hình AI tạo sinh khác được xây dựng dựa trên mô hình khuếch tán. Mô hình này gồm hai mạng nơ-ron: mạng tạo sinh (generator) và mạng phân biệt (discriminator). Mạng tạo sinh tạo ra các mẫu dữ liệu giả, trong khi mạng phân biệt cố gắng phân biệt giữa dữ liệu thật (từ dữ liệu gốc) và dữ liệu giả (do mạng tạo sinh tạo ra).\nTrong quá trình đào tạo, mạng tạo sinh liên tục cải thiện khả năng tạo ra dữ liệu giống thật hơn, trong khi mạng phân biệt ngày càng giỏi hơn trong việc nhận biết thật và giả. Quá trình đối nghịch này tiếp diễn cho đến khi mạng tạo sinh có thể tạo ra dữ liệu mà mạng phân biệt không thể phân biệt được với dữ liệu thật.\nƯu điểm: Có thể tạo ra các mẫu chất lượng cao\nNhược điểm: Độ đa dạng của mẫu tạo ra có thể hạn chế, phù hợp hơn cho các dữ liệu cụ thể\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.2/",
	"title": "Thiết lập môi trường",
	"tags": [],
	"description": "",
	"content": "Khởi động môi trường Để thiết lập môi trường cho hội thảo, bạn mở môi trường AWS Cloud9, như được trình bày chi tiết trong các hướng dẫn sau.\nĐiều hướng tới trang: AWS Cloud9.\nTrong phần Environments, chọn bedrock-workshop-environment-c9, bên dưới Cloud9 IDE, hãy chọn Open.\nThiết lập môi trường Bạn cần Stack name để thực hiện các bước tiếp theo. Bạn có thể thực hiện các bước sau:\nĐiều hướng đến AWS CloudFormation\nTruy xuất Stack name như hình mẫu.\nHoặc bạn có thể sao chép tên của Stack name tại đây: bedrock-workshop-environment\nSau khi lấy được Stack name, hãy thay thế bằng tên đã sao chép và chạy lệnh sau. Hãy đảm bảo cấu hình biến môi trường S3BucketName, thùng này được sử dụng để lưu trữ các tài liệu mẫu và thử nghiệm hội thảo. export CFNStackName = \u0026lt;your-startup-stack-name\u0026gt; export S3BucketName=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;S3BucketName\u0026#39;].OutputValue\u0026#34; --output text) echo \u0026#34;S3 bucket name: $S3BucketName\u0026#34; Chạy các lệnh trên trong môi trường Cloud9 IDE Chúc mừng thành công của bạn khi đã đến được đây! Mời bạn đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.7/2.7.2/",
	"title": "Triển khai hàm RAG Lambda",
	"tags": [],
	"description": "",
	"content": "Triển khai Mở thiết bị đầu cuối Cloud9, chạy lệnh sau để xây dựng và triển khai với mã lambda mới được cập nhật. cd ~/environment/bedrock-serverless-workshop sam build \u0026amp;\u0026amp; sam deploy Trong nhiệm vụ này, bạn có cơ hội làm việc với các mô hình LLM sau, mỗi mô hình cung cấp các chức năng và chuyên môn hóa độc đáo. Các mô hình này bao gồm: anthropic.claude-3-haiku - Claude 3 Haiku là mô hình nhanh nhất và nhỏ gọn nhất của Anthropic để phản hồi gần như ngay lập tức. Haiku là lựa chọn tốt nhất để xây dựng trải nghiệm AI liền mạch mô phỏng tương tác của con người. anthropic.claude-3-5-sonnet - Mô hình thông minh và tiên tiến nhất của Anthropic, Claude 3.5 Sonnet, thể hiện các khả năng đặc biệt trên nhiều nhiệm vụ và đánh giá khác nhau. anthropic.claude-3-opus - Opus là mô hình cực kỳ thông minh với hiệu suất đáng tin cậy trên các nhiệm vụ phức tạp. Nó có thể điều hướng các lời nhắc mở và các tình huống chưa từng thấy với sự trôi chảy đáng kinh ngạc và khả năng hiểu giống con người. Sử dụng Opus để tự động hóa các tác vụ và đẩy nhanh quá trình nghiên cứu và phát triển trên nhiều trường hợp sử dụng và ngành công nghiệp khác nhau. mistral.mistral-7b-instruct - Mistral là một mô hình ngôn ngữ lớn có hiệu quả cao được tối ưu hóa cho các tác vụ dựa trên ngôn ngữ có khối lượng lớn, độ trễ thấp. Các trường hợp sử dụng phổ biến của Mistral là tóm tắt văn bản, cấu trúc hóa, trả lời câu hỏi và hoàn thành mã meta.llama3-1-8b-instruct - Llama 3.1 8B phù hợp nhất với năng lực tính toán và tài nguyên hạn chế. Mô hình này vượt trội trong việc tóm tắt văn bản, phân loại văn bản, phân tích tình cảm và dịch ngôn ngữ đòi hỏi suy luận độ trễ thấp. Quay lại trình duyệt và mở trang web chatbot. Nhấp vào liên kết RAG Chat with LLMs . Trang web được hiển thị như sau Chúc mừng thành công của bạn khi đã đến được đây! Mời bạn đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.5/2.5.2/",
	"title": "Triển khai và cập nhật code lambda",
	"tags": [],
	"description": "",
	"content": "Triển khai và cập nhật code lambda Mở Cloud9 terminal, chạy lệnh sau để xây dựng và triển khai với mã lambda mới được cập nhật. cd ~/environment/bedrock-serverless-workshop sam build \u0026amp;\u0026amp; sam deploy Bạn có thể kiểm tra hàm Lambda đã cập nhật bằng cách sử dụng AWS Lambda Console.\nTrong nhiệm vụ này, bạn có cơ hội làm việc với các mô hình LLM sau, mỗi mô hình cung cấp các chức năng và chuyên môn độc đáo. Các mô hình này bao gồm: anthropic.Claude-3-haiku Claude 3 Haiku là mô hình nhanh nhất và nhỏ gọn nhất của Anthropic để phản hồi gần như ngay lập tức. Haiku là lựa chọn tốt nhất để xây dựng trải nghiệm AI liền mạch mô phỏng tương tác của con người.\nanthropic.claude-3-5-sonnet Mô hình thông minh và tiên tiến nhất của Anthropic, Claude 3.5 Sonnet, thể hiện các khả năng đặc biệt trong nhiều nhiệm vụ và đánh giá khác nhau.\nanthropic. Claude-3-opus Opus là mô hình cực kỳ thông minh với hiệu suất đáng tin cậy trên các nhiệm vụ phức tạp. Nó có thể điều hướng các lời nhắc mở và các tình huống chưa từng thấy với sự trôi chảy đáng kinh ngạc và khả năng hiểu giống con người. Sử dụng Opus để tự động hóa các tác vụ và đẩy nhanh quá trình nghiên cứu và phát triển trên nhiều trường hợp sử dụng và ngành công nghiệp khác nhau.\nmistral.mistral-7b-instruct Mistral là một mô hình ngôn ngữ lớn có hiệu quả cao được tối ưu hóa cho các tác vụ dựa trên ngôn ngữ có khối lượng lớn, độ trễ thấp. Các trường hợp sử dụng phổ biến cho Mistral là tóm tắt văn bản, cấu trúc hóa, trả lời câu hỏi và hoàn thành mã.\nmeta.llama3-1-8b-instruct - Llama 3.1 8B phù hợp nhất với sức mạnh tính toán và tài nguyên hạn chế. Mô hình này vượt trội trong việc tóm tắt văn bản, phân loại văn bản, phân tích tình cảm và dịch ngôn ngữ đòi hỏi suy luận độ trễ thấp.\nMở trang website chatbot. Nếu bạn chưa đăng nhập, hãy sử dụng thông tin đăng nhập mà bạn đã lấy trước đó để đăng nhập. Trang web được hiển thị như sau: Chúc mừng thành công của bạn khi đã đến được đây! Mời bạn đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.3/",
	"title": "Các mô hình tiêu biểu",
	"tags": [],
	"description": "",
	"content": "Hội thảo này sẽ giới thiệu đến bạn 4 mô hình tiêu biểu của generative AI bao gồm:\nMô hình tự mã hóa biến phân (VAEs)\nMạng đối nghịch tạo sinh (GANs)\nMô hình khuếch tán (Diffusion Models)\nMô hình tăng cường truy xuất (RAG)\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/3-clean-up/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Dọn dẹp tài nguyên Sau khi hoàn tất hội thảo, để ngăn ngừng phát sinh chi phí, bạn nên xóa các tài nguyên đã tạo trong tài khoản AWS của mình.\nĐiều hướng tới trang AWS Cloud9, sau đó chạy các lệnh sau: cd ~/environment/bedrock-serverless-workshop sam delete Để xóa ngăn xếp SAM, hãy chạy lệnh sau. aws cloudformation delete-stack --stack-name $CFNStackName "
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.3/1.3.3/",
	"title": "Mô hình khuếch tán",
	"tags": [],
	"description": "",
	"content": "Mô hình khuếch tán (Diffusion Models) Khái niệm: Mô hình khuếch tán, còn được gọi là mô hình xác suất khuếch tán khử nhiễu (DDPMs), là một loại mô hình tạo sinh. Chúng hoạt động theo hai bước: khuếch tán (forward diffusion) và khử nhiễu (reverse denoising).\nƯu điểm: Mô hình khuếch tán có thể tạo ra các lớp rất phức tạp và chất lượng cao\nNhược điểm: Quá trình đào tạo và tạo dữ liệu mới có thể mất nhiều thời gian\nTrong quá trình khuếch tán, các thay đổi cực nhỏ (nhiễu) ngẫu nhiên được thêm vào dữ liệu để huấn luyện dần. Còn quá trình khử nhiễu sẽ loại bỏ các nhiễu này để tạo ra một mẫu dữ liệu mới giống với bản gốc. Mô hình bắt đầu từ các nhiễu ngẫu nhiên hoàn toàn và từ từ loại bỏ nó để tạo ra dữ liệu mới có chất lượng cao.\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.3/",
	"title": "Triển khai ứng dụng Amazon Bedrock Serverless",
	"tags": [],
	"description": "",
	"content": "Trong tác vụ này, bạn thực hiện các bước để xây dựng và triển khai các dịch vụ phụ trợ. Việc triển khai tạo ra một serverless application, trong đó UI sẽ gọi các lệnh gọi API dựa trên REST để gọi API Amazon Bedrock. Sau khi mã được triển khai, nó sẽ khởi chạy Amazon API Gateway và một hàm AWS Lambda. Để thiết lập môi trường của bạn.\nTriển khai ứng dụng hãy mở môi trường AWS Cloud9.\nSao chép mã nguồn, hãy chạy lệnh sau.\ncd ~/environment git clone https://github.com/aws-samples/bedrock-serverless-workshop.git Nâng cấp và xây dựng ứng dụng Để nâng cấp và xây dựng phần phụ trợ và phần giao diện của ứng dụng không có máy chủ, hãy chạy lệnh sau. cd ~/environment/bedrock-serverless-workshop chmod +x startup.sh ./startup.sh Tập lệnh này giúp thực hiện các tác vụ sau: Nâng cấp hệ điều hành và cài đặt phần mềm iq.\nXây dựng phần phụ trợ bằng cách sử dụng sam build.\nTriển khai phần phụ trợ bằng cách sử dụng sam deploy.\nCài đặt Amplify và xây dựng phần giao diện.\nXuất bản ứng dụng giao diện bằng cách sử dụng Amplify.\nTập ​​lệnh sẽ mất từ ​​2 đến 5 phút để hoàn thành. Nếu có cửa sổ bật lên cảnh báo git tại một thời điểm nào đó, hãy chọn OK.\nTrong quá trình amplify add host, bạn sẽ được nhắc chọn hai lần, giữ nguyên các lựa chọn mặc định và nhấn enter. Các lựa chọn mặc định là: Hosting with Amplify Console (Managed hosting with custom domains, Continuous deployment)\nManual deployment\nSau khi hoàn tất, bạn sẽ thấy hình ảnh sau. Sao chép và lưu trữ giá trị cho URL amplifyapp, user_id và password trong trình soạn thảo văn bản. Bạn sử dụng các thông tin xác thực này để đăng nhập vào giao diện hệ thống. amplifyapp: Đường dẫn URL gúp bạn khởi chạy giao diện của hệ thống.\nuser_id và password: Giúp bạn đăng nhập vào được hệ thống.\nGiao diện này vẫn chưa sẵn sàng với các tài liệu nguồn và tính năng trò chuyện vẫn chưa hoạt động. Trong nhiệm vụ tiếp theo, bạn sẽ được hướng dẫn để hoàn tất việc lập chỉ mục các tài liệu nguồn và kiểm tra bằng các câu hỏi mẫu\nTrước khi chuyển sang nhiệm vụ tiếp theo, hãy chạy các lệnh bên dưới, đây là các biến môi trường cần thiết cho lệnh xây dựng SAM và Amplify cho phần còn lại của phòng thí nghiệm. export SAMStackName=\u0026#34;sam-$CFNStackName\u0026#34; export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) export KendraIndexID=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;KendraIndexID\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CognitoUserPool\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolClientId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CongnitoUserPoolClientID\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) Chúc mừng thành công của bạn khi đã đến được đây! Mời bạn đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.4/",
	"title": "Hướng dẫn sử dụng Amazon Bedrock Console",
	"tags": [],
	"description": "",
	"content": "Mở Amazon Bedrock Console Đăng nhập vào AWS Management Console, sau đó nhập Bedrock vào hộp tìm kiếm của bảng điều khiển. Bạn được điều hướng đến trang có giao diện như sau.\nKích hoạt quyền truy cập mô hình Theo mặc định, tài khoản AWS của bạn không có quyền truy cập mô hình và bạn cần truy cập các mô hình Claude3, Mistral và Llama để hoàn thành hội thảo này.\nỞ góc trên bên trái của bảng điều khiển Amazon Bedrock, hãy chọn biểu tượng menu (hình bánh hamburger). Trong ngăn điều hướng, hãy chọn Quyền truy cập mô hình, sau đó chọn Bật các mô hình cụ thể.\nChọn các mô hình Claude 3 Sonnet, Claude 3 Haiku, Llama 3 Chat 13B, và Mistral 7B Instruct.\nChọn Tiếp theo.\nChọn Gửi.\nThiết lập model Ở góc trên bên trái của bảng điều khiển Amazon Bedrock, hãy chọn biểu tượng menu. Trong mục Playgrounds, hãy chọn Chat.\nTại mục Playgrounds, hãy chọn Select model.\nTại mục Category, hãy chọn Anthropic và chọn Claude 3 Haiku.\nChọn Apply để áp dụng model.\nĐặt câu hỏi Giao diện Chat playground\u0026quot; được mở ra. Trong hộp văn bản nhắc, hãy nhập câu hỏi, sau đó chọn Run. Ví dụ: Liệt kê 10 thành phố đông dân nhất Hoa Kỳ?\nMô hình phân tích và trả về phản hồi như hình bên dưới.\nChúc mừng thành công của bạn khi đã đến được đây! Mời bạn đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/1-introduction/1.3/1.3.4.1/",
	"title": "Mô hình tăng cường truy xuất",
	"tags": [],
	"description": "",
	"content": "Mô hình tăng cường truy xuất (RAG) Khái niệm: RAG - Retrieval-Augmented Generation là một kiến trúc mô hình kết hợp giữa hai thành phần chính: cơ chế retrieval (truy xuất thông tin) và generation (tạo văn bản). Nó được thiết kế để cải thiện chất lượng đầu ra của mô hình ngôn ngữ bằng cách truy xuất các tài liệu hoặc dữ liệu liên quan từ một cơ sở kiến thức trước khi tạo văn bản. RAG được sử dụng phổ biến trong các ứng dụng hỏi đáp, tìm kiếm tri thức và các hệ thống AI thông minh khác.\nRetrieval (Truy xuất thông tin): Khi nhận đầu vào (ví dụ: câu hỏi), hệ thống sẽ tìm kiếm các tài liệu hoặc đoạn văn bản liên quan từ một cơ sở dữ liệu hoặc hệ thống tìm kiếm thông tin. Generation (Tạo văn bản): Sau khi truy xuất các tài liệu, hệ thống sử dụng mô hình ngôn ngữ (ví dụ: GPT, BERT) để tạo ra văn bản đầu ra dựa trên thông tin đã truy xuất được. Ưu điểm: RAG kết hợp khả năng truy xuất dữ liệu với việc tạo văn bản, cho phép mô hình tổng hợp thông tin từ các nguồn bên ngoài, giúp nâng cao độ chính xác và khả năng cập nhật thông tin. Nó giúp giảm độ lệch, tránh các suy luận không chính xác và không cần huấn luyện lại khi có dữ liệu mới. Điều này làm cho RAG hiệu quả trong các hệ thống hỏi đáp và xử lý dữ liệu lớn.\nNhược điểm: RAG phụ thuộc vào chất lượng dữ liệu truy xuất, có thể dẫn đến câu trả lời sai nếu dữ liệu không chính xác. Nó cũng đòi hỏi chi phí tính toán cao, làm tăng độ phức tạp và độ trễ. Hơn nữa, nếu không quản lý tốt cơ sở dữ liệu, hệ thống có thể gặp khó khăn trong việc truy xuất thông tin liên quan và duy trì tính nhất quán khi các tài liệu có mâu thuẫn.\nBằng việc tương tác với các FM của Generative AI thông qua lời nhắc. Lời nhắc chủ yếu được sử dụng để giao tiếp với các FM chuyển văn bản thành văn bản. Trong lời nhắc, chúng ta sẽ cung cấp hướng dẫn cho FM tạo nội dung mới.\nViết lời nhắc tốt giúp nhận được phản hồi tốt nhất từ ​​các FM. Ví dụ về lời nhắc bao gồm câu hỏi, hướng dẫn hoặc mô tả chi tiết. Kỹ thuật lời nhắc có thể bao gồm việc diễn đạt truy vấn, chỉ định một kiểu đầu ra nhất định, cung cấp bối cảnh có liên quan và chỉ định vai trò (chẳng hạn như \u0026ldquo;hành động như một trợ lý con người\u0026rdquo;) cho AI. Bạn sẽ khám phá thêm về kỹ thuật lời nhắc trong hội thảo này.\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.5/",
	"title": "Trò chuyện chung với nhiều LLM",
	"tags": [],
	"description": "",
	"content": "Trò chuyện chung với nhiều LLM Bạn đã triển khai thành công ứng dụng chatbot không máy chủ Amazon Bedrock, sử dụng AWS SAM cho phần phụ trợ và AWS Amplify cho phần giao diện người dùng. Phần giao diện người dùng đóng vai trò là giao diện người dùng cơ bản để kiểm tra giải pháp với nhiều câu hỏi và tham số nhắc khác nhau. Trong phần này, bạn sẽ cập nhật và triển khai hàm LLM Lambda, hàm này cho phép trò chuyện chung với nhiều mô hình ngôn ngữ lớn.\nHãy mở AWS Cloud9 environment và nhấp vào Open link. "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.6/",
	"title": "Lập chỉ mục dữ liệu nguồn bằng cách sử dụng Amazon Kendra",
	"tags": [],
	"description": "",
	"content": "Trong kiến ​​trúc RAG, Amazon Kendra có thể được tận dụng để lập chỉ mục và tìm kiếm thông qua bộ sưu tập các tài liệu mẫu được lưu trữ trong Amazon S3 và các nguồn khác. Bạn có thể cung cấp truy vấn hoặc đặt câu hỏi và Kendra sẽ thực hiện tìm kiếm tương tự trên toàn bộ nội dung được lập chỉ mục để xác định thông tin có liên quan nhất.\nKhả năng xử lý ngôn ngữ tự nhiên tiên tiến của Kendra cho phép hiểu ý định và truy vấn của bạn, sau đó truy xuất nội dung có liên quan nhất từ ​​các nguồn dữ liệu được lập chỉ mục. Điều này cho phép bạn nhanh chóng tìm thấy thông tin họ cần mà không cần phải sàng lọc thủ công khối lượng lớn tài liệu.\nBằng cách tích hợp Kendra vào giai đoạn \u0026ldquo;Truy xuất\u0026rdquo; của kiến ​​trúc RAG, các tổ chức có thể nâng cao khả năng tìm kiếm và khám phá thông tin tổng thể của mình, cuối cùng hỗ trợ phân tích hiệu quả hơn và tạo ra thông tin chi tiết và phản hồi. Việc tích hợp liền mạch của Kendra với Amazon S3 giúp đơn giản hóa quy trình lập chỉ mục và quản lý nội dung cơ bản, biến nó thành một công cụ mạnh mẽ trong khuôn khổ RAG.\nCài đặt tài liệu mẫu Cài đặt một số tài liệu mẫu để thử nghiệm giải pháp này. Trong có 03 tài liệu mẫu: Tài liệu về biên bản cuộc họp tháng 9 năm 2023 của Ủy ban Thị trường Mở Liên bang (FOMC).\nTài liệu về báo cáo Phát triển Bền vững của Amazon năm 2022.\nTài liệu về báo cáo 10K của Henry Schein, một nhà cung cấp dịch vụ nha khoa.\nBạn có thể tải xuống hoặc sử dụng bất kỳ tài liệu nào để thử nghiệm giải pháp này hoặc bạn có thể mang dữ liệu của riêng mình để tiến hành thử nghiệm.\nĐể sao chép các mẫu nhắc nhở và tài liệu mẫu mà bạn đã tải xuống thùng S3, hãy chạy lệnh sau. cd ~/environment/bedrock-serverless-workshop mkdir sample-documents curl https://www.federalreserve.gov/monetarypolicy/files/fomcminutes20230920.pdf --output sample-documents/fomcminutes20230920.pdf curl https://sustainability.aboutamazon.com/2022-sustainability-report.pdf --output sample-documents/2022-sustainability-report-amazon.pdf curl https://investor.henryschein.com/static-files/fcf569ec-fdbb-4591-b73d-0d1d849efd78 --output sample-documents/2022-hs1-10k.pdf Câu lệnh sẽ thực hiện như hình bên dưới Cài đặt các tài liệu mẫu và mẫu nhắc nhở Hãy kiểm tra lại Stack name để đảm báo không có tiến trình gián đoạn trước đó, hãy chạy lệnh sau (ở dây là Stack name của bạn) export CFNStackName = \u0026lt;your-startup-stack-name\u0026gt; export S3BucketName=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;S3BucketName\u0026#39;].OutputValue\u0026#34; --output text) echo \u0026#34;S3 bucket name: $S3BucketName\u0026#34; Để sao chép các mẫu nhắc nhở và tài liệu mẫu mà bạn đã tải xuống thùng S3, hãy chạy lệnh sau. cd ~/environment/bedrock-serverless-workshop aws s3 cp sample-documents s3://$S3BucketName/sample-documents/ --recursive aws s3 cp prompt-engineering s3://$S3BucketName/prompt-engineering/ --recursive Các câu lệnh trên sẽ thực hiện như hình bên dưới Sau khi tải lên thành công, hãy xem lại bảng điều khiển Amazon S3 và mở bucket. Bạn sẽ thấy nội dung như sau Lập chỉ mục các tài liệu mẫu bằng cách sử dụng Amazon Kendra Chỉ mục Amazon Kendra và nguồn dữ liệu Amazon S3 đã được tạo trong quá trình cung cấp ban đầu cho hội thảo này. Trong tác vụ này, bạn lập chỉ mục tất cả các tài liệu trong nguồn dữ liệu S3.\nĐăng nhập vào AWS Management Console, sau đó nhập Kendra vào hộp tìm kiếm của bảng điều khiển. Xem lại để đảm bảo rằng tài khoản AWS của bạn có quyền truy cập vào Amazon Kendra, sau đó chọn region AWS nơi hội thảo này đang diễn ra. Chọn Indexes\nTại bảng chọn bên trái, hãy chọn Data sources\nĐể bắt đầu lập chỉ mục tất cả các tài liệu từ thư mục sample-documents, hãy chọn S3DocsDataSource, sau đó chọn Sync now. Việc lập chỉ mục có thể mất vài phút. Đợi cho đến khi hoàn tất. Để truy vấn chỉ mục Amazon Kendra bằng một vài câu hỏi mẫu, trong ngăn điều hướng bên trái, hãy chọn Search indexed content, sau đó đặt một câu hỏi. Đặt câu hoỉ ví dụ, như: What is federal funds rate as of April 2024 ? . Và Amazon Kendra phân tích và trả về câu trả lời như hình bên dưới Chúc mừng thành công của bạn khi đã đến được đây! Mời bạn đến với phần tiếp theo của hội thảo. "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.7/",
	"title": "Trò chuyện RAG với nhiều LLM",
	"tags": [],
	"description": "",
	"content": "Bạn đã triển khai thành công ứng dụng chatbot không máy chủ Amazon Bedrock và cho phép mô hình truy cập vào các Mô hình ngôn ngữ lớn (LLM) cần thiết bằng bảng điều khiển Amazon Bedrock. Ngoài ra, bạn đã hoàn tất thiết lập Amazon Kendra.\nGiải pháp dựa trên RAG với Amazon Bedrock và LangChain Giải pháp trong hội thảo này được xây dựng bằng cách sử dụng phương pháp Retrieval-Augmented Generation (RAG). RAG là một kiến ​​trúc mô hình tích hợp các khía cạnh của cả kỹ thuật truy xuất và tạo để nâng cao chất lượng và tính liên quan của văn bản được tạo. Khi bạn nhập câu hỏi vào hộp văn bản câu hỏi, các bước sau đây sẽ được chạy để cung cấp cho bạn câu trả lời có nguồn gốc từ nguồn tài liệu:\nTruy xuất: Quy trình này tìm kiếm trong một khối dữ liệu văn bản lớn để tìm thông tin hoặc ngữ cảnh có liên quan. Trong giai đoạn này, Amazon Kendra lấy câu hỏi từ yêu cầu và tìm kiếm các câu trả lời và tài liệu tham khảo có liên quan.\nTăng cường: Sau khi truy xuất thông tin có liên quan, mô hình sử dụng ngữ cảnh đã truy xuất để tăng cường việc tạo văn bản. Điều này có nghĩa là văn bản được tạo ra chịu ảnh hưởng của thông tin đã truy xuất, đảm bảo rằng nội dung được tạo ra phù hợp với ngữ cảnh và mang tính thông tin.\nTạo: Tạo trong RAG đề cập đến khía cạnh tạo truyền thống của mô hình, trong đó mô hình tạo văn bản mới dựa trên ngữ cảnh đã truy xuất và tăng cường. Văn bản được tạo ra này có thể ở dạng câu trả lời, phản hồi hoặc giải thích.\nLangChain: Để sắp xếp luồng này, chúng tôi sử dụng tác nhân LangChain trong hội thảo này. Các bản tóm tắt linh hoạt và bộ công cụ toàn diện của LangChain trao quyền cho các nhà phát triển khai thác khả năng của các mô hình nền tảng (FM).\nTrong nhiệm vụ này, bạn sẽ triển khai hàm RAG (Lấy, Phân tích, Tạo) Lambda để cung cấp trải nghiệm chatbot theo ngữ cảnh với các tập dữ liệu của bạn. Các tập dữ liệu mẫu được lưu trữ trong Amazon S3 và được lập chỉ mục bằng Amazon Kendra. Để sắp xếp luồng giữa các truy vấn của người dùng, chỉ mục Kendra và LLM, bạn sẽ sử dụng LangChain làm công cụ sắp xếp. Mã Lambda được cung cấp sử dụng API LangChain để tóm tắt logic phức tạp cần thiết cho tích hợp này.\nBằng cách tận dụng sức mạnh của Amazon Bedrock, Amazon Kendra và LangChain, bạn có thể tạo trải nghiệm chatbot liền mạch và theo ngữ cảnh cho người dùng của mình, cho phép họ tương tác với các tập dữ liệu của bạn theo cách tự nhiên và hiệu quả.\nTrong phần này, bạn sẽ cập nhật và triển khai hàm RAG Lambda, hàm này cho phép trò chuyện theo ngữ cảnh với nhiều mô hình ngôn ngữ lớn.\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.8/",
	"title": "Kỹ thuật nhanh chóng",
	"tags": [],
	"description": "",
	"content": "Kỹ thuật nhắc nhở Kỹ thuật nhắc nhở là một khía cạnh quan trọng của kiến ​​trúc Retrieval-Augmented Generation (RAG), vì nó đóng vai trò quan trọng trong từng giai đoạn của quy trình. Trong giai đoạn Retrieval, kỹ thuật nhắc nhở được sử dụng để tạo ra các truy vấn hiệu quả nhằm truy xuất thông tin có liên quan nhất từ ​​các nguồn dữ liệu, chẳng hạn như tài liệu Amazon S3 được lập chỉ mục bằng Amazon Kendra, nắm bắt chính xác ý định của người dùng.\nTrong giai đoạn phân tích, các nhắc nhở hướng dẫn quá trình xử lý và trích xuất thông tin chi tiết từ dữ liệu đã truy xuất, chỉ định loại phân tích, mức độ chi tiết và định dạng đầu ra mong muốn. Cuối cùng, trong giai đoạn Generation, kỹ thuật nhắc nhở rất cần thiết để tận dụng các mô hình ngôn ngữ lớn như Claude 3 Sonnet để tạo ra các phản hồi mạch lạc, có liên quan và cụ thể cho từng tác vụ, cung cấp bối cảnh, hướng dẫn và chỉ dẫn cần thiết. Bằng cách thành thạo kỹ thuật nhắc nhở, người dùng có thể tối ưu hóa hiệu suất của kiến ​​trúc RAG, đảm bảo rằng thông tin đã truy xuất có liên quan, phân tích có hiểu biết sâu sắc và các phản hồi được tạo ra giải quyết hiệu quả nhu cầu của người dùng.\nTrong nhiệm vụ này, bạn sẽ làm việc để tinh chỉnh lời nhắc cho mô hình ngôn ngữ lớn Claude 3 Sonnet. Bạn sẽ thử nghiệm cách hành vi phản hồi thay đổi dựa trên mẫu nhắc nhở bạn cung cấp. Ban đầu, bạn sẽ đặt một câu hỏi với một lời nhắc có cấu trúc lỏng lẻo, điều này có thể khiến LLM đi chệch khỏi ngữ cảnh và đưa ra kết quả không mong muốn. Khi bạn tinh chỉnh lời nhắc, phản hồi sẽ trở nên phù hợp hơn với kết quả mong đợi và ít gây ảo giác hơn. Phần này vẫn là một phần của kiến ​​trúc RAG và sẽ sử dụng cùng một nguồn dữ liệu mà bạn đã sử dụng trong nhiệm vụ trước đó.\n"
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.8/2.8.1/",
	"title": "Trải nghiệm",
	"tags": [],
	"description": "",
	"content": "Mở trang kỹ năng nhanh chóng Quay lại trình duyệt và mở trang web chatbot. Nhấp vào liên kết Prompt Engineering. Trang web được hiển thị như sau Hãy thử một câu hỏi bên dưới không liên quan đến các tài liệu nguồn mà bạn đã lập chỉ mục cho đến nay: Hãy kể cho tôi nghe một câu chuyện về một con cáo và một con hổ, câu chuyện phải dành cho trẻ em 5 tuổi và dưới 100 từ.\nĐối với mẫu lời nhắc, hãy sao chép lời nhắc sau và dán vào hộp văn bản Mẫu lời nhắc . Đây là một lời nhắc có cấu trúc lỏng lẻo mà không có bất kỳ chỉ dẫn hoặc hướng dẫn cụ thể nào được cung cấp cho mô hình Claude 3 Sonnet.\n{context} and {question} "
},
{
	"uri": "http://levuxuananit.github.io/vi/2-hand-on/2.8/2.8.2/",
	"title": "Bổ sung cấu trúc lời nhắc",
	"tags": [],
	"description": "",
	"content": "Cấu trúc lời nhắc Đối với mẫu lời nhắc, hãy sao chép lời nhắc sau và dán vào hộp văn bản Mẫu lời nhắc . Đây là một lời nhắc có cấu trúc lỏng lẻo mà không có bất kỳ chỉ dẫn hoặc hướng dẫn cụ thể nào được cung cấp cho mô hình Claude 3 Sonnet. {context} and {question} Bây giờ, hãy cập nhật lời nhắc bằng phiên bản tinh chỉnh sau. Với lời nhắc tinh chỉnh này, bạn có thể mong đợi nhận được đầu ra mong muốn. Con người: Bạn là cố vấn AI thông minh và cung cấp câu trả lời cho các câu hỏi bằng cách sử dụng thông tin dựa trên thực tế. Sử dụng các thông tin sau để cung cấp câu trả lời ngắn gọn cho câu hỏi được bao gồm trong thẻ \u0026lt;câu hỏi\u0026gt;. Tìm thông tin ngữ cảnh được bao gồm trong thẻ \u0026lt;bối cảnh\u0026gt;. Nếu bạn không biết câu trả lời, chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời. \u0026lt;bối cảnh\u0026gt;{bối cảnh}\u0026lt;/bối cảnh\u0026gt; \u0026lt;câu hỏi\u0026gt;{câu hỏi}\u0026lt;/câu hỏi\u0026gt; Câu trả lời phải cụ thể và chỉ sử dụng thông tin thực tế. "
},
{
	"uri": "http://levuxuananit.github.io/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://levuxuananit.github.io/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]